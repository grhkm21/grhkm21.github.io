<!DOCTYPE html>
<html lang="en-uk">


<script async src="https://www.googletagmanager.com/gtag/js?id=G-6BP6QGT61V"></script>
<script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'G-6BP6QGT61V');
</script>

<title>Nonlinear Regression (1) | Blog by grhkm</title>
<meta charset="utf-8">
<meta name="generator" content="Hugo 0.109.0">
<meta name="description" content="Short article on nonlinear regression, focusing on polynomial models. Written for HS students.">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<link rel="stylesheet"
    href="https://grhkm21.github.io/css/index.css">
<link rel="canonical" href="https://grhkm21.github.io/posts/nonlinear-regression/">
<link rel="alternate" type="application/rss+xml" href=""
    title="Blog by grhkm">

<script src="https://code.jquery.com/jquery-3.6.4.min.js"
    integrity="sha256-oP6HI9z1XaZNBrJURtCoUT5SUnxFr8s3BzRl+cbzUq8=" crossorigin="anonymous"></script>
<script defer src="https://grhkm21.github.io/js/spoiler.js"></script>




<link rel="stylesheet" href="/scss/hint.min.css">


<link rel="stylesheet" href="/scss/alert.min.css">


<link rel="stylesheet" href="/scss/toc.min.css">


<link rel="stylesheet" href="https://grhkm21.github.io/katex/katex.min.css">
<script defer src="https://grhkm21.github.io/katex/katex.min.js"></script>
<script defer src="https://grhkm21.github.io/katex/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body)"></script>

<link rel="stylesheet" href="https://grhkm21.github.io/katex/katex.min.css">
<script defer src="https://grhkm21.github.io/katex/katex.min.js"></script>
<script defer src="https://grhkm21.github.io/katex/contrib/auto-render.min.js"></script>
<script>
    document.addEventListener("DOMContentLoaded", function () {
        renderMathInElement(document.body, {
            
            
            delimiters: [
                { left: '$$', right: '$$', display: true },
                { left: '$', right: '$', display: false },
                { left: '\\(', right: '\\)', display: false },
                { left: '\\[', right: '\\]', display: true }
            ],
            
            throwOnError: false
        });
    });
</script>

<header>
    
    <a href="https://grhkm21.github.io/" class="title">Blog by grhkm</a>
    
    
    <nav>
        
        <a href="/about-me/">About</a>
        
        <a href="/">Posts</a>
        
    </nav>
    
</header>

<article>
    <header>
        <h1>Nonlinear Regression (1)</h1>
    </header>

    

    <h1 id="notes-on-non-linear-regression">Notes on Non-Linear Regression</h1>
<h2 id="1-linear-regression">1. Linear Regression</h2>
<p>To begin, let&rsquo;s recall how linear regression is done. We are given $n$ <strong>data points</strong> $(x_1, y_1), (x_2, y_2), \ldots, (x_n, y_n)$, and we are to make a linear <strong>model</strong></p>
<p>$$
f(x) = ax + b
$$</p>
<p>Such that the model &ldquo;closely approximates&rdquo; the data points, that is, to have $y_i \sim f(x_i)$. The <strong>residual</strong> is defined as the absolute difference between the interpolated data and the actual data:</p>
<p>$$
r_i := y_i - f(x_i) = y_i - (ax_i + b)
$$</p>
<p>To mathematically define &ldquo;closely approximates&rdquo;, we will need the notion of error measures. There are many types of such measures, such as averaging the absolute value of the residuals or using the maximum residual. However, in this page we will use the following measure, analogous to <em>standard deviation</em> in Statistics:</p>
<p>$$
E_2(f) := \sqrt{\frac{1}{n}\sum_{i=1}^n r_i^2}
$$</p>
<p>The functional $E_2$ is called the Root Mean Square error, abbreviated RMS below. With this notion, we can formalise the problem of linear regression:</p>

<div class="alert alert-info" role="alert"><p><strong>Linear Regression:</strong> Given $n$ points $(x_1, y_1), (x_2, y_2), \ldots, (x_n, y_n)$, find coefficients $a, b$ such that for the model $f(x) = ax + b$, the RMS</p>
<p>$$
E_2(f) = \sqrt{\frac{1}{n}\sum_{i=1}^n (y_i - f(x_i))^2}
$$</p>
<p>is minimised.</p>
</div>

<hr>
<h2 id="2-polynomial-regression">2. Polynomial Regression</h2>
<p>With the formalisation, it is easy to generalise to non-linear models. For the purpose of this document, we shall focus on <em>polynomial</em> models, as follows:</p>

<div class="alert alert-success" role="alert"><p><strong>(Degree-$k$) Polynomial Regression:</strong> Given $n$ points $(x_1, y_1), (x_2, y_2), \ldots, (x_n, y_n)$, find coefficients $a_0, a_1, \ldots, a_k$ such that for the model</p>
<p>$$
f(x) = a_0 + a_1x + a_2x^2 + \ldots + a_kx^k = \sum_{i=0}^k a_ix^i
$$</p>
<p>, the RMS</p>
<p>$$
E_2(f) = \sqrt{\frac{1}{n}\sum_{i=1}^n (y_i - f(x_i))^2}
$$</p>
<p>is minimized.</p>
</div>

<p>To help understand, here is an example.</p>
<p><strong>Example 1:</strong> Suppose that we have four data points $(0, 1), (1, 5), (2, 7), (4, 4)$, and we wish to closely model this with a quadratic model. Then, $k = 2$ and we wish to find coefficients $a_0, a_1, a_2$ such that</p>
<p>$$
\begin{align*}
E_2(f) &amp;= \sqrt{\frac{1}{n}\sum_{i=1}^n(y_i - f(x_i))^2} \\ &amp;= \sqrt{\frac{1}{4}\left((1 - f(0))^2 + (5 - f(1))^2 + (7 - f(2))^2 + (4 - f(4))^2\right)} \\ &amp;= \sqrt{\frac{1}{4}(4a_0^2 + 14a_0a_1 + 21a_1^2 + 42a_0a_2 + 146a_1a_2 + 273a_2^2 - 34a_0 - 70a_1 - 194a_2 + 91)}
\end{align*}
$$</p>
<p>is minimised. Of course it looks impossible to solve right now, as we have $3$ variables and a total of $10$ terms to minimise together. However, we will solve the minimisation problem in section 4.</p>
<p><img src="https://i.imgur.com/0lRE0Zk.png" alt=""></p>
<hr>
<h2 id="3-lagrange-multiplier">3. Lagrange Multiplier</h2>
<p>To minimise the absolute mess above, we will require another tool. Recall that in normal calculus, it is find the local minimum any function, say $f(x) = x^3 + 3x^2 - 5$ - we differentiate it to get $f&rsquo;(x) = 3x^2 + 6x$, then find it&rsquo;s zeroes $f&rsquo;(x) = 3x(x + 2) \implies x = 0, -2$, and finally check that $f&rsquo;&rsquo;(x) &gt; 0$ to get that $x = 0$ is a local minimum of $f$. We wish to do something like that with more than one variables!</p>
<p>This is when Lagrange can help us. In the 18th and early 19th century, Lagrange developed a tool called the Lagrange Multiplier to solve such optimisation problems. The details of <em>when</em> it is applicable is complicated, but for our purpose, we will present a simplified version.</p>
<p>Firstly, we need a notion of &ldquo;derivative&rdquo; when there are more than one variable. It will be simplier to demonstrate by an example than to rigorously define it. Take a function $f(x, y) = x^2 + 5xy + 3y$. We define the <em>partial derivative with respect to $x$</em>, denoted $\frac{\partial f}{\partial x}$ as taking the normal derivative, except that we treat every variable except $x$ as a constant. In this case, this means $y$ is a constant (like $2$ is), and we get</p>
<p>$$\frac{\partial f}{\partial x} = \underbrace{2x}_{x^2} + \underbrace{5y}_{5xy} + \underbrace{0}_{3y}$$</p>
<p>Similarly, we can also do this for other variables:</p>
<p>$$\frac{\partial f}{\partial y} = \underbrace{0}_{x^2} + \underbrace{5x}_{5xy} + \underbrace{3}_{3y}$$</p>
<p>As you see, this is essentially the same as the normal derivatives - we simply fix one variable and ignore the other variables.</p>
<p>With this, we can state what Lagrange derived:</p>

<div class="alert alert-info" role="alert"><p><strong>Theorem (Simplified):</strong> Suppose that $f$ is a function in terms of $x_1, x_2, \ldots, x_k$. Then, at where $f(x_1, x_2, \ldots, x_k)$ achieves local minimum or maximum, we must have</p>
<p>$$
\frac{\partial f}{\partial x_1} = \frac{\partial f}{\partial x_2} = \cdots = \frac{\partial f}{\partial x_k} = 0
$$</p>
</div>

<p>For example, with our function of $f(x, y) = x^2 + 5xy + 3y$, we can find it&rsquo;s local extremas by setting $2x + 5y = 5x + 3 = 0$ and solving the equations.</p>
<p>With this, we can go back and solve the minimization problem in Section 2.</p>
<hr>
<h2 id="section-4-finding-the-model">Section 4: Finding the Model</h2>
<p>In Section 2, we arrived at the formalisation of the problem, which requires us to minimise the following scary looking formula:</p>
<p>$$
E_2(f) = \sqrt{\frac{1}{n}\sum_{i=1}^n (y_i - f(x_i))^2}
$$</p>
<p>To do this, first notice that it suffices to minimise the term $\sum_{i=1}^n (y_i - f(x_i))^2$, since multiplying by a constant $\frac{1}{n}$ and taking square root does not change the minimality of the term. Hence, we minimise</p>
<p>$$
R_2(f) = \sum_{i=1}^n (y_i - f(x_i))^2
$$</p>
<p>where $f(x) = a_0 + a_1x + \cdots + a_kx^k = \sum_{j=0}^k a_jx^j$. Now applying Lagrange&rsquo;s Theorem, we require</p>
<p>$$
\frac{\partial R_2}{\partial a_0} = \frac{\partial R_2}{\partial a_1} = \cdots = \frac{\partial R_2}{\partial a_k} = 0
$$</p>
<p>What does each of these terms equal to? Well, for a particular index $j$,</p>
<p>$$
\frac{\partial R_2}{\partial a_j} = \frac{\partial}{\partial a_j} \sum_{i=1}^n (\color{red}{y_i - f(x_i)})^{\color{cyan}{2}}
$$</p>
<p>We can apply the chain rule on the second term. Recall that the normal chain rule states $\frac{d}{dx} f(g(x)) = f&rsquo;(g(x))\cdot g&rsquo;(x)$. Here, it is exactly the same. Apply $f(x) = x^2$ and $g(x) = y_i - f(x_i)$,</p>
<p>$$
\frac{\partial R_2}{\partial a_j} = \sum_{i=1}^n \underbrace{\color{cyan}{2(\color{red}{y_i - f(x_i)})}}_{f&rsquo;(g)}\cdot\underbrace{\frac{\partial}{\partial a_j}(\color{red}{y_i - f(x_i)})}_{g&rsquo;} = -\sum_{i=1}^n 2(y_i - f(x_i))\cdot\frac{\partial f}{\partial a_j}(x_i)
$$</p>
<p>Finally, we know that $f(x) = a_0 + a_1x + \cdots + a_kx^k$. Therefore, if we take partial derivative w.r.t. $a_i$, which means treating every other variable as constant, then we are left with $\frac{\partial f}{\partial a_j} = \frac{\partial}{\partial a_j}(a_ix^i + \text{constant}) = x^i$. Therefore,</p>
<p>$$
\frac{\partial R_2}{\partial a_j} = -2\sum_{i=1}^n (y_i - f(x_i)) x_i^j
$$</p>
<p>And hence, our problem reduces to solving</p>
<p>$$
\sum_{i=1}^n (y_i - f(x_i)) = \sum_{i=1}^n (y_i - f(x_i))x_i = \cdots = \sum_{i=1}^n (y_i - f(x_i))x_i^k = 0
$$</p>
<p>This still looks scary, but let&rsquo;s look back at our example.</p>
<p><strong>Example 1 (Cont.):</strong> We had ${(0, 1), (1, 5), (2, 7), (4, 4)}$ as our data points, and $f(x) = a_0 + a_1x + a_2x^2$. Therefore, we want to solve the folllowing system of equations:</p>
<p>$$
\begin{cases}\begin{align*}
&amp;\sum_{i=1}^n (y_i - f(x_i)) &amp;= 17 - 4a_0 - 7a_1 - 21a_2 = 0 \\
&amp;\sum_{i=1}^n (y_i - f(x_i))x_i &amp;= 35 - 7a_0 - 21a_1 - 73a_2 = 0 \\
&amp;\sum_{i=1}^n (y_i - f(x_i))x_i^2 &amp;= 97 - 21a_0 - 73a_1 - 273a_2 = 0 \\
\end{align*}\end{cases}
$$</p>
<p>Since there are three variables and three equations, we can solve this using normal system of equations! Indeed, we get $(a_0, a_1, a_2) = \left(\frac{107}{110}, \frac{1147}{220}, -\frac{49}{44}\right) \approx (0.97, 5.21, -1.11)$, which matches the coefficients of Desmos.</p>
<hr>
<h2 id="5-code">5. Code</h2>
<p>To aid computation, we coded up a program in a programming language called Sage. It is a library extension of Python. In Sage, solving linear equations are easy: we first transform the coefficients into a <code>Matrix</code>, then call the function <code>.solve_right</code> to solve the equations - see <a href="https://doc.sagemath.org/html/en/tutorial/tour_linalg.html">docs</a> for more details.</p>
<p>Anyways, here is the final code:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">find_coeffs</span>(points, k):
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># arguments:</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># points: [(x_1, y_1), ..., (x_n, y_n)] in an array</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># k: degree of the polynomial model</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># return values:</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># - coefficients a_0, a_1, ..., a_k in an array</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># - also the RMS of the model, E_2(f)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> len(points) <span style="color:#f92672">&lt;</span> k <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">raise</span> <span style="color:#a6e22e">RuntimeError</span>(<span style="color:#e6db74">&#34;Not enough data points.&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># computing coefficients of a_j</span>
</span></span><span style="display:flex;"><span>    x_coeff_matrix <span style="color:#f92672">=</span> Matrix(QQ, k <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>, k <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>    x_sum_of_powers <span style="color:#f92672">=</span> [sum(x<span style="color:#f92672">^</span>i <span style="color:#66d9ef">for</span> x, y <span style="color:#f92672">in</span> points) <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">2</span> <span style="color:#f92672">*</span> k <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>)]
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(k <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> j <span style="color:#f92672">in</span> range(k <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>):
</span></span><span style="display:flex;"><span>            x_coeff_matrix[i, j] <span style="color:#f92672">=</span> x_sum_of_powers[i <span style="color:#f92672">+</span> j]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># computing the constant terms of y_i</span>
</span></span><span style="display:flex;"><span>    y_coeff_matrix <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(k <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>):
</span></span><span style="display:flex;"><span>        y_coeff_matrix<span style="color:#f92672">.</span>append(sum(y <span style="color:#f92672">*</span> x<span style="color:#f92672">^</span>i <span style="color:#66d9ef">for</span> x, y <span style="color:#f92672">in</span> points))
</span></span><span style="display:flex;"><span>    y_coeff_matrix <span style="color:#f92672">=</span> vector(QQ, y_coeff_matrix)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># solving system of linear equations</span>
</span></span><span style="display:flex;"><span>    coef <span style="color:#f92672">=</span> x_coeff_matrix<span style="color:#f92672">.</span>solve_right(y_coeff_matrix)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># computing residuals and RMS</span>
</span></span><span style="display:flex;"><span>    f <span style="color:#f92672">=</span> <span style="color:#66d9ef">lambda</span> x: sum(a_i <span style="color:#f92672">*</span> x<span style="color:#f92672">^</span>i <span style="color:#66d9ef">for</span> i, a_i <span style="color:#f92672">in</span> enumerate(coef))
</span></span><span style="display:flex;"><span>    residuals <span style="color:#f92672">=</span> [y_i <span style="color:#f92672">-</span> f(x_i) <span style="color:#66d9ef">for</span> x_i, y_i <span style="color:#f92672">in</span> points]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    RMS <span style="color:#f92672">=</span> sqrt(sum(r<span style="color:#f92672">^</span><span style="color:#ae81ff">2</span> <span style="color:#66d9ef">for</span> r <span style="color:#f92672">in</span> residuals) <span style="color:#f92672">/</span> len(points))
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> coef<span style="color:#f92672">.</span>n(), RMS<span style="color:#f92672">.</span>n()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">main</span>():
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># data points</span>
</span></span><span style="display:flex;"><span>    arr <span style="color:#f92672">=</span> [(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>), (<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">5</span>), (<span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">7</span>), (<span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">4</span>)]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># linear regression</span>
</span></span><span style="display:flex;"><span>    coef, RMS <span style="color:#f92672">=</span> find_coeffs(arr, <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">&#34;coef =&#34;</span>, coef)
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">&#34;RMS =&#34;</span>, RMS)
</span></span><span style="display:flex;"><span>    print()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># quadratic regression</span>
</span></span><span style="display:flex;"><span>    coef, RMS <span style="color:#f92672">=</span> find_coeffs(arr, <span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">&#34;coef =&#34;</span>, coef)
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">&#34;RMS =&#34;</span>, RMS)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">if</span> __name__ <span style="color:#f92672">==</span> <span style="color:#e6db74">&#34;__main__&#34;</span>:
</span></span><span style="display:flex;"><span>    main()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">Output:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">Linear Regression:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">- coef = (3.20000000000000, 0.600000000000000)
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">- RMS = 1.97484176581315
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">Quadratic Regression:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">- coef = (0.972727272727273, 5.21363636363636, -1.11363636363636)
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">- RMS = 0.0476731294622796
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">&#34;&#34;&#34;</span>
</span></span></code></pre></div><hr>
<h2 id="6-future">6. Future</h2>
<p>It is easy to extend the work to other models or using other error measures other than $E_2$, and I would love to investigate this further in other fields such as the $p$-adic integers.</p>

</article>



</html>